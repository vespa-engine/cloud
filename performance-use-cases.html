---
# Copyright Verizon Media. All rights reserved.
title: "Performance Use Cases"
layout: page-wide
---

<style>
  h2 {
    margin-top: 0px;
  } <!-- align better with icons  -->
</style>

<!-- https://denali-design.github.io/denali-icon-font/docs/ for font used as icons below -->

<p class="intro">
  Vespa Cloud: Performance use cases
</p>
<!-- ToDo: more/better text in intro -->
<!-- ToDo: this belongs in vespa.ai more than in cloud site -->

<section id="high-performance-content-engine" class="bg-white">
  <div class="container">
    <div class="row">
      <div class="col-lg-2 text-center">
        <span class="d-icon is-large denali-icon-color d-dashboard-add"/>
      </div>
      <div class="col-lg-10">
        <h2>High-performance Content Engine</h2>
        <p>
          Vespa is built for really large realtime serving, and supports "unlimited" content node (proton) size.
          <a href="https://docs.vespa.ai/documentation/proton.html">Proton</a>
          is a C++ component and does not have memory limitations other than restrictions on
          <a href="https://docs.vespa.ai/documentation/attributes.html">attributes</a> -
          a common use case is running in 256G containers.
          It has its own memory allocator called
          <a href="https://github.com/vespa-engine/vespa/tree/master/vespamalloc">vespa-malloc</a>.
        </p><p>
          Usages vary from applications with tens of billions of documents and a moderate query rate (example: image search)
          to millions of documents with query rates in 100.000/second (example: ad serving).
          Vespa supports <a href="https://docs.vespa.ai/documentation/elastic-vespa.html#grouped-distribution">performance groups</a>
          for flexible replica placement to enable a wide range of use cases.
          All cases support a sustained, high throughput for updating documents.
      </p><p>
          Vespa supports a wide range of ML models by transforming them to
        <a href="https://docs.vespa.ai/documentation/tensor-user-guide.html">tensors</a> -
          and uses LLVM for high-performance ranking.
      </p><p>
          Read more in <a href="https://docs.vespa.ai/documentation/performance/">Vespa Performance</a>.
        </p>
        <h3>Highlighted features</h3>
        <ul>
          <li>Huge content node memory support, including vespa-malloc</li>
          <li>Tensor representation</li>
          <li>Performance groups</li>
          <li>Sustained throughput for document partial updates</li>
          <li>Data structures uses chunked memory - low peak-to-average gives better utilization,
            lower costs and small compaction footprint</li>
          <li>LLVM</li>
        </ul>
      </div>
    </div>
  </div>
</section>

<section id="graceful-peak-load-handling" class="bg-light-blue">
  <div class="container">
    <div class="row">
      <div class="col-lg-2 text-center">
        <span class="d-icon is-large denali-icon-color d-cloud-check"/>
      </div>
      <div class="col-lg-10">
        <h2>Graceful peak load handling</h2>
        <p>
          It is hard to size an application for the highest possible load peak.
          Unexpected things happen.
          Instead of allocating idle resources for peak loads that almost never happen,
          a good tradeoff is degrading relevance quality, requiring less coverage.
          This keeps cost under control, still serving useful results during high peaks.
        </p>
        <h3>Highlighted features</h3>
        <ul>
          <li><a href="https://docs.vespa.ai/documentation/graceful-degradation.html">
            Coverage / Ranking degradation</a></li>
          <li>Soft timeout returns results at timeout - <em>some</em> results are better than none</li>
        </ul>
      </div>
    </div>
  </div>
</section>

<section id="multi-threaded-queries" class="bg-light-blue">
  <div class="container">
    <div class="row">
      <div class="col-lg-2 text-center">
        <span class="d-icon is-large denali-icon-color d-network-role"/>
      </div>
      <div class="col-lg-10">
        <h2>Multi-threaded queries</h2>
        <p>
          Most engines are multi-threaded to fully utilize the computing resources.
          In Vespa, the data layout on disk is fully orthogonal to threads used per query.
          It is hence easy to increase number of threads used per query without having to redistribute data.
          Balance capacity requirements, query latency and throughput by tuning
          <a href="https://docs.vespa.ai/documentation/reference/schema-reference#num-threads-per-search">num-threads-per-search</a>.
        </p>
        <h3>Highlighted features</h3>
        <ul>
          <li>True multi-thread queries</li>
        </ul>
      </div>
    </div>
  </div>
</section>

<section/>
