---
title: Automated Deployments
---

<p>
This document explains the process — and safety mechanisms —
which allows application changes and Vespa platform releases to be continuously deployed to production.
</p><p>
Each application package build which is submitted to Vespa Cloud constitutes an application change which
must be tested and, if found healthy, deployed.
Similarly, each change to the Vespa platform, by the Vespa team, must be tested and deployed for all the hosted applications.
Vespa Cloud automates all these tests and deployments, with features including:
<ul>
  <li>chained runs of tests and deployment, with retries of failed jobs;</li>
  <li>multiple concurrent instances of an application in each zone, upgraded as specified by the user,
    for testing application changes in a subset of the service before rolling them out further;</li>
  <li>separation of application and platform changes, making it easier to pinpoint breaking changes
    (application changes are always allowed when an upgrade fails, as they may be necessary to fix the breakage);</li>
  <li>cancellation of any current application roll-out, upon submission of a new application revision;</li>
  <li>throttling of platform upgrades, to detect unhealthy upgrades with a subset of applications; and</li>
  <li>cancellation of platform upgrades which are found unhealthy, across all applications.</li>
</ul>
With <em>Continuous Integration (CI)</em> <!-- link to something later -->
that builds and submits changes to the application as they are committed, Vespa Cloud thus provides
full-fledged Continuous Deployment (CD) of all its applications, both for application developers, and for the Vespa team.
</p>



<h2 id="setting-up-deployment-to-production">Setting up deployment to production</h2>
<p>
Follow steps in <a href="getting-to-production">Getting to Production</a>.
Also see the <a href="/reference/vespa-cloud-api.html">API reference</a>. Summary:
<ol>
  <li><strong>Create deployment.xml</strong>
  <p>
    Configure where and when to deploy — <a href="/reference/zones">supported zones</a>.
  </p>
  </li><li><strong>Create system and staging tests</strong>
  <p>
    Create at least one <a href="/reference/testing#system-tests">system</a>
    and one <a href="/reference/testing#staging-tests">staging</a> test.
  </p>
  </li><li><strong>Set up a deployment job</strong>
  <p>
    Configure a job in a tool like Github Actions -
    see <a href="#continuous-deployment">Continuous Deployment</a>
  </p>
  </li>
</ol>
Once set up, make changes to the production application
simply by checking in the change to the application source repository.
</p>



<h2 id="continuous-deployment">Continuous Deployment</h2>
<p>
In the continuous build tool, set up a job which builds the Vespa application
and ships it to the Vespa cloud.
Trigger this job by a merge to the main branch in the source control system,
where the Vespa application repository is stored.
</p><p>
The deployment job must have access to the <a href="/reference/vespa-cloud-api#common-properties">API key</a> -
e.g. a <em>secret</em> in GitHub, see ToDo example.
</p><p>
The job should execute something like the following (modify as needed if not using git):
<pre>
$ mvn clean vespa:compileVersion

$ mvn -P fat-test-application \
  package vespa:submit \
  -Dvespa.compile.version="$(cat target/vespa.compile.version)"  \
  -Drepository=$(git config --get remote.origin.url) \
  -Dbranch=$(git rev-parse --abbrev-ref HEAD) \
  -Dcommit=$(git rev-parse HEAD) \
  -DauthorEmail=$(git log -1 --format=%aE)
</pre>
Track deployment at
<em>https://console.vespa.oath.cloud/tenant/mytenant/application/myapp/deployment</em> -
click "Deployment" in the console / refresh page.
</p>



<h2 id="deployment-orchestration">Deployment orchestration</h2>
<p>
Vespa applications are <em>compiled</em> against one version of the Vespa Java artifacts, 
and then deployed to nodes in the cloud where the <em>runtime</em> Vespa version is controlled by the system. 
This runtime, or <em>platform</em>, version is also continuously updated, independently of application updates. 
This leads to a number of possible combinations of application packages and platform versions for each application.
</p><p>
Instead of a simple <em>pipeline</em>, Vespa deployments are <em>orchestrated</em> such that any deployment of an application package
<code>X</code> to a production cluster with platform version <code>Y</code> is preceded by <em>system</em> and <em>staging</em>
tests using the same version pair; and likewise for any upgrade of the platform to version <code>Y</code> of a production
cluster running an application package <code>X</code>.
Good system and staging tests therefore guard against both unfortunate changes in the application, and in the Vespa platform. 
<em><a href="#system-tests">System</a> and <a href="#staging-tests">staging</a> tests are mandatory</em>; see below for how to write them.
</p><p>
When an application or platform change has been successfully verified in a system and staging tests,
it is deployed to a <a href="/reference/zones">production zone</a>.
This deployment job may also contain verification tests that need to succeed before the change rolls on to more zones.
Good production tests fail if a change is deployed in production which impacts the observed behavior of the application negatively,
typically by asserting on application metrics after a delay.
If the application is deployed in multiple prod zones,
this makes it possible to revert to the old version quickly by shifting traffic to another production zone.
</p><p>
Status of ongoing tests and deployments is found by clicking <em>Deployment</em> in the application view in the
<a href="http://console.vespa.ai">console</a>. <!--It is possible to delay the deployment to a region
by clicking the <em>pause</em> button, and to force through a deployment by clicking the <em>deploy</em> button. -->
Examples of advanced deployment configuration which can be set in <a href="/reference/deployment">deployment.xml</a> include:
<ul>
  <li>Deployment order and parallelism</li>
  <li>Time windows with no deployments</li>
  <li>Grace periods between deployments, and before their tests</li>
</ul>
</p>




<h2 id="production-deployments">Production deployments</h2>
<p>
Production jobs run sequentially by default, but can be configured to run in parallel,
in <a href="/reference/deployment">deployment.xml</a>;
inside each zone, Vespa itself orchestrates the deployment, such that the application may continue to serve,
even as subsets of its nodes are down for upgrade.
A production deployment job is not complete before the upgrade is complete on all nodes,
and the cluster has returned to a stable state.
When the Vespa platform is upgraded, each node has to restart with the new runtime;
this is typically slower than an application change by the user,
which often amounts only to a reconfiguration of smaller parts of the deployment.
</p>





<h2 id="deleting-an-application">Deleting an application</h2>
<p>
<ol>
    <li>Remove all instances in <a href="/reference/deployment#instance">deployment.xml</a>, then run the CI job.
    <a href="/getting-to-production#removing-a-production-instance">Details</a>.</li>
  <li>Delete the application in the <a href="http://console.vespa.ai">console</a>.</li>
  <li>Delete the CI job that builds and pushes new artifacts.</li>
</ol>
</p>



<h2 id="feature-switches-and-bucket-tests">Feature switches and bucket tests</h2>
<p>
With CD, it is not possible to hold off releasing a feature until it is done,
test it manually until convinced it works and then release it to production.
What to do instead?
The answer is <em>feature switches</em>: release new features to production as they are developed,
but include logic which keeps them deactivated until they are ready,
or until they have been verified in production with a subset of users.
</p><p>
<em>Bucket tests</em> is the practice of systematically testing new features or behavior for a controlled subset of users.
This is common practice when releasing new science models,
as they are difficult to verify in test, but can also be used for other features.
</p><p>
To test new behavior in Vespa, use a combination of
<a href="https://docs.vespa.ai/documentation/chained-components">search chains</a>
and
<a href="https://docs.vespa.ai/documentation/reference/search-definitions-reference#rank-profile">rank profiles</a>,
controlled by
<a href="https://docs.vespa.ai/documentation/query-profiles">query profiles</a>,
where one query profiles correspond to one bucket.
These features support inheritance to make it easy to express variation without repetition.
</p><p>
Some times a new feature require
<a href="http://docs.vespa.ai/documentation/reference/search-definitions-reference#modify-search-definitions">
incompatible changes to a data field</a>.
To be able to CD such changes, it is necessary to create a new field containing the new version of the data.
This costs extra resources but less than the alternative: standing up a new system copy with the new data.
New fields can be added and populated while the system is live.
<p></p>
It should be mentioned that the need for incompatible changes can be decreased by making the semantics of the fields more precise.
E.g., if a field is defined as the "quality" of a document, where a higher number means higher quality,
a new algorithm which produces a different range and distribution will typically be an incompatible change.
However, if the field is defined more precisely as the average time spent on the document once it is clicked,
then a new algorithm which produces better estimates of this value will not be an incompatible change.
Using precise semantics also have the advantage of making it easier to understand
if the use of the data and its statistical properties are reasonable.
</p>



<h2 id="integration_testing">Integration testing</h2>
<p>
Another challenge with CD is integration testing across multiple services:
another service depends on this Vespa application for its own integration testing.
There are two ways to provide this: Either create an additional <em>application instance</em> for testing
or use test data in the production instance.
Using test data in production requires that some thought is given to separating this data from the real
data in queries.
A separate instance gives complete isolation,
but with some additional overhead,
and may not produce quite as realistic testing of queries,
as those will run only over the test data in the separate instance.
</p>
