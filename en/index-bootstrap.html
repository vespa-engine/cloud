---
# Copyright Verizon Media. All rights reserved.
title: Index bootstrap
layout: page
---


<style>
  td {
    vertical-align: top;
  }
</style>

<p>
  When bootstrapping an index, one must consider node resource configuration and number of nodes.
  The strategy is to iterate:
</p>
<img src="../assets/index-bootstrap.svg" width="465"/>
<ol class="list">
  <li>Feed smaller chunks of data</li>
  <li>Evaluate</li>
  <li>Deploy new node counts / node resource configuration</li>
  <li>Wait for data migration to complete</li>
  <li>Evaluate</li>
</ol>
<p>
  While doing this, ensure the cluster is <strong>never more than 50% full</strong> -
  this gives headroom to later increase/shrink the index
  and change schema configuration easily using automatic reindexing.
  It is easy to downscale resources after the bootstrap,
  and it saves a lot of time keeping the clusters within limits - hence max 50%.
</p>
<p>
  Review the <a href="https://docs.vespa.ai/en/overview.html">Vespa Overview</a> to understand
  the different between <em>container</em> and <em>content</em> clusters before continuing.
</p>



<h2 id="preparations">Preparations</h2>
<p>
  The content node resource configuration should not have ranges for index bootstrap,
  as autoscaling will interfere with the evaluation in this step.
  This is a good starting point, <strong>make sure there are no ranges like [2,3]</strong>:
</p>
<pre>
&lt;nodes count="2"&gt;
    &lt;resources vcpu="2.0" memory="8Gb" disk="50Gb"/&gt;
&lt;/nodes&gt;
</pre>
<p>
  To evaluate how full the content cluster is, use <a href="monitoring">metrics</a> from content nodes - example:
</p>
<!-- ToDo: verify this is the correct metrics -->
<pre>
$ curl --cert data-plane-public-cert.pem --key data-plane-private-key.pem \
  https://vespacloud-docsearch.vespa-team.aws-us-east-1c.public.vespa.oath.cloud/metrics/v2/values | \
  jq '.nodes[] | select(.role|startswith("content/documentation")) |
  {   hostname:    .hostname,
      "disk.util": .node.metrics[] | select(.values."disk.util") | .values."disk.util",
      "mem.util":  .node.metrics[] | select(.values."mem.util")  | .values."mem.util"
  }'

...

{
  "hostname": "h2564a.prod.aws-us-east-1c.vespa-external.aws.oath.cloud",
  "disk.util": 0.646856704,
  "mem.util": 8.3981990814209
}
{
  "hostname": "h2550a.prod.aws-us-east-1c.vespa-external.aws.oath.cloud",
  "disk.util": 0.648052736,
  "mem.util": 8.228063583374
}
</pre>
<p>
  Metrics parsing notes:
</p>
<ul class="list">
  <li>Get the endpoint from the console, append <code>/metrics/v2/values</code> -
    this dumps metrics from all nodes</li>
  <li>Using <a href="https://stedolan.github.io/jq">jq</a> (or other JSON parsing tool),
    select nodes from a named content cluster (replace with current name), here called <em>documentation</em></li>
  <li>Print relevant metrics from the node</li>
</ul>
<p>
  Once able to dump the metrics above, you are ready to bootstrap the index.
</p>
<p>
  For performance, feed using the <a href="https://docs.vespa.ai/en/vespa-http-client.html">vespa-http-client</a> -
  download the client and test it using your own endpoints and schema:
</p>
<pre>
$ curl -L -o vespa-http-client-jar-with-dependencies.jar \
    https://search.maven.org/classic/remotecontent?filepath=com/yahoo/vespa/vespa-http-client/7.391.28/vespa-http-client-7.391.28-jar-with-dependencies.jar

$ cat docs.json
{
    "put": "id:mynamespace:music::123",
    "fields": {
        "album": "A Head Full of Dreams",
        "artist": "Coldplay",
        "year": 2015,
        "category_scores": {
            "cells": [
                { "address" : { "cat" : "pop" },  "value": 1 },
                { "address" : { "cat" : "rock" }, "value": 0.2 },
                { "address" : { "cat" : "jazz" }, "value": 0 }
            ]
        }
    }
}

$ ENDPOINT=https://vespacloud-docsearch.vespa-team.aws-us-east-1c.public.vespa.oath.cloud/

$ java -jar vespa-http-client-jar-with-dependencies.jar \
    --useTls --certificate data-plane-public-cert.pem --privateKey data-plane-private-key.pem \
    --endpoint $ENDPOINT \
    --file docs.json

$ curl --cert data-plane-public-cert.pem --key data-plane-private-key.pem \
    $ENDPOINT/document/v1/mynamespace/music/docid/
{
  "pathId": "/document/v1/mynamespace/music/docid/",
  "documents": [
    {
      "id": "id:mynamespace:music::123",
      "fields": {
        "artist": "Coldplay",
        "year": 2015,
        "category_scores": {
          "cells": [
            { "address": { "cat": "pop" }, "value": 1 },
            { "address": { "cat": "rock" }, "value": 0.20000000298023224 },
            { "address": { "cat": "jazz" },"value": 0 }
          ]
        },
        "album": "A Head Full of Dreams"
      }
    }
  ],
  "continuation": "AAAACAAAAAAAAAAGAAAAAAAAAAUAAAAAAAABAAAAAAEgAAAAAAAAoAAAAAAAAAAA",
  "documentCount": 1
}
</pre>



<h2 id="bootstrap">Bootstrap</h2>
<table class="table">
  <thead>
  <th>Step</th><th>Description</th>
  </thead>
  <tbody>
  <tr>
    <td><strong>1% feed</strong></td>
    <td>
      <p>
        The purpose of this step is to feed a tiny chunk of the corpus to:
      </p>
        <ol class="list">
          <li>Estimate the memory and disk resource configuration</li>
          <li>Estimate the number of nodes required for the 10% step</li>
        </ol>
      <p>
        Feed a small data set, observing the util metrics, stop no later than 50% memory/disk util.
        The resource configuration should be modified so disk is in the 50-80% range of memory.
        Example: if memory util is 50%, disk util should be 30-45%.
        The reasoning is that memory is a more expensive component than disk,
        better over-allocate on disk and just track memory usage.
      </p>
      <p>
        Look at memory util. Say the 1% feed caused a 15% memory util -
        this means that the 10% feed will take 150%, or 3X more than the 50% max.
        There are two options, either increase memory/disk or add more nodes.
        A good rule of thumb at this stage is that the final 100% feed could fit on 4 or more nodes,
        and there is a 2-node minimum for redundancy.
        The default configuration at the start of this document is quite small,
        so a 3X at this stage means triple the disk and memory,
        and add more nodes in later steps.
      </p>
      <p>
        Deploy changes (if needed).
        Whenever node count increases or resource configuration is modified, new nodes are added,
        and data is migrated to new nodes. Example: growing from 2 to 3 nodes means each of the 2 current nodes
        will migrate 33% of their data to the new node.
        Read more in <a href="https://docs.vespa.ai/en/elastic-vespa.html">Elastic Vespa</a>.
        It saves time to let the cluster finish data migration before feeding more data.
        In this step it will be fast as the data volume is small,
        but nevertheless check the <code>vds.idealstate.merge_bucket.pending.average</code> metric.
        Wait for 0 for all nodes - this means data migration is completed:
      </p>
<pre>
$ curl --cert data-plane-public-cert.pem --key data-plane-private-key.pem \
  https://vespacloud-docsearch.vespa-team.aws-us-east-1c.public.vespa.oath.cloud/metrics/v2/values?consumer=Vespa | \
  jq '.nodes[] | select(.role|startswith("content/documentation")) |
    .services[] | select (.name=="vespa.distributor") |
      .metrics[].values."vds.idealstate.merge_bucket.pending.average"'

...

  0
  0
</pre>
      <p>
        At this point, you can validate that both memory and disk util is less than 5%,
        so the 10% feed will fit.
      </p>
    </td>
  </tr>
  <tr>
    <td><strong>10% feed</strong></td>
    <td>
      <p>
        Feed the 10% corpus, still observing util metrics.
      </p>
      <p>
        As the content cluster capacity is increased,
        it is normal to eventually be CPU bound in the container or content cluster.
        To find CPU utilization, add this to the JSON print in the <a href="#preparations">preparations</a>:
<pre>
"cpu.util": .node.metrics[] | select(.values."cpu.util") | .values."cpu.util",
</pre>
        For container clusters, replace with <code>select(.role|startswith("container/default"))</code>
        (assuming the cluster is named <em>default</em>).
      </p>
      <p>
        A 10% feed is a great baseline for the full capacity requirements.
        Fine tune the resource config and number of hosts as needed.
        If you deploy changes, wait for the
        <code>vds.idealstate.merge_bucket.pending.average</code> metric to go to zero again.
        This now takes longer time as nodes are configured larger,
        it normally completes within a few hours.
      </p>
      <p>
        Again validate memory and disk util is less than 5% before the full feed.
      </p>
    </td>
  </tr>
  <tr>
    <td><strong>100% feed</strong></td>
    <td>
      <p>
        Feed the full data set, observing the metrics.
        You should be able to estimate timing by extrapolation, this is linear at this scale.
        At feed completion, observe the util metrics for the final fine tuning.
      </p>
      <p>
        A great exercise at this point is to add a node then reduce a node, and take the time to completion
        (<code>vds.idealstate.merge_bucket.pending.average</code> to 0).
        This is useful information when the application is in production,
        as you know the time to add or shrink capacity in advance.
      </p>
      <p>
        It can be a good idea to reduce node count to get the memory util closer to 70% at this step,
        to optimize for cost.
        However, do not spend too much time optimizing in this step, next step is normally
        <a href="https://docs.vespa.ai/en/performance/sizing-search.html">sizing for query load</a>.
        This will again possibly alter resource configuration and node counts / topology,
        but now you have a good grasp at how to easiest bootstrap the index for these experiments.
      </p>
    </td>
  </tr>
  </tbody>
</table>



<h2 id="troubleshooting">Troubleshooting</h2>
<p>
  Make sure you are able to feed and access documents as the example in <a href="#preparations">preparations</a>.
  Read <a href="security-model.html">security model</a> for cert/key usage.
</p>
<p>
  Feeding too much causes a <a href="https://docs.vespa.ai/en/reads-and-writes.html#feed-block">feed blocked</a> state.
  Add a node to the full content cluster in services.xml, and wait for data migration to complete -
  i.e. wait for the <code>vds.idealstate.merge_bucket.pending.average</code> metric to go to zero.
  It is better to add a node than increasing node resources, as data migration is quicker.
</p>



<h2 id="further-reading">Further reading</h2>
<ul>
  <li><a href="https://docs.vespa.ai/en/reads-and-writes.html">Reads and Writes</a></li>
  <li><a href="https://docs.vespa.ai/en/performance/sizing-feeding.html">Vespa Feed Sizing Guide</a></li>
  <li><a href="https://cloud.vespa.ai/en/benchmarking">Vespa Cloud Benchmarking</a></li>
  <li><a href="https://cloud.vespa.ai/en/monitoring">Monitoring</a></li>
</ul>
